const { parseMetadataFromContent } = require('../utils/metadata');
const { pdfToMarkdownViaHtml, processPdfText } = require('../utils/pdf-utils');
const { getSparseVector, generateDocumentHash } = require('../utils/text-utils');
const { throwIfAborted } = require('./ollama-agent');
const { extractCSV, extractXLSX, extractPPTX, extractRTF } = require('../utils/office-extractors');

function createDocumentService({
  qdrantClient,
  embeddingService,
  piiService,
  categorizationService,
  visionService,
  descriptionService,
  libreOfficeConverter,
  pdfParse,
  pdf2md,
  mammoth,
  piiDetectionEnabled,
  categorizationModel,
  visionEnabled,
  autoGenerateDescription
}) {
  async function processSingleFile(file, collectionName, autoCategorize = false, options = {}) {
    throwIfAborted(options.signal);

    const reportStage = (stage) => {
      if (typeof options.onStage === 'function') {
        try {
          options.onStage(stage);
        } catch {
          // ignore
        }
      }
    };

    reportStage('Preparing fileâ€¦');

    // Extract cloud metadata if provided
    const cloudMetadata = options.cloudMetadata || {};
    const { s3Key, driveId } = cloudMetadata;

    // Decode base64-encoded filename (used to preserve UTF-8 for Hebrew/Arabic/etc)
    let filename = file.originalname;

    // Note: For multiple file upload, filename_encoded should be per-file
    // For now, we'll use the originalname directly and handle encoding in the client
    if (/[\x80-\xFF]/.test(filename)) {
      // Fallback: try to decode if contains non-ASCII
      try {
        const decoded = Buffer.from(filename, 'latin1').toString('utf8');
        if (!decoded.includes('\uFFFD')) {
          filename = decoded;
        }
      } catch (e) {
        console.warn('Failed to decode filename:', e.message);
      }
    }

    const fileExt = filename.split('.').pop().toLowerCase();

    console.log(`Processing file: ${filename} (${fileExt})`);

    let content = '';
    let metadata = {};

    // Parse file based on type
    switch (fileExt) {
      case 'txt':
        reportStage('Reading textâ€¦');
        content = file.buffer.toString('utf8');
        break;

      case 'json':
        reportStage('Reading JSONâ€¦');
        const jsonData = JSON.parse(file.buffer.toString('utf8'));
        // If JSON has content field, use it; otherwise stringify
        content = jsonData.content || JSON.stringify(jsonData, null, 2);
        // Extract metadata from JSON if present
        if (jsonData.metadata) metadata = jsonData.metadata;
        if (jsonData.category) metadata.category = jsonData.category;
        if (jsonData.location) metadata.location = jsonData.location;
        if (jsonData.tags) metadata.tags = jsonData.tags;
        break;

      case 'pdf':
        reportStage('Extracting PDF textâ€¦');
        // Try PDF.js â†’ HTML â†’ Markdown approach
        console.log('Converting PDF using HTML intermediate format...');

        try {
          content = await pdfToMarkdownViaHtml(file.buffer);
          console.log('âœ“ PDF converted via HTML â†’ Markdown successfully');
        } catch (htmlError) {
          console.warn('PDF via HTML conversion failed, trying @opendocsg/pdf2md:', htmlError.message);

          // Fallback 1: Try @opendocsg/pdf2md
          try {
            content = await pdf2md(file.buffer);
            console.log('âœ“ PDF converted with @opendocsg/pdf2md');
          } catch (pdf2mdError) {
            console.warn('pdf2md failed, using custom text processing:', pdf2mdError.message);

            // Fallback 2: Basic text extraction with processing
            const pdfData = await pdfParse(file.buffer);
            content = processPdfText(pdfData.text);
            console.log('âœ“ PDF processed with basic text extraction');
          }
        }

        // Get page count
        const pdfData = await pdfParse(file.buffer);
        metadata.pages = pdfData.numpages;
        break;

      case 'docx':
        reportStage('Extracting DOCX textâ€¦');
        // Convert to markdown to preserve headings, lists, tables, etc.
        const docxResult = await mammoth.convertToMarkdown({ buffer: file.buffer });
        content = docxResult.value;
        console.log('DOCX converted to markdown successfully');
        break;

      case 'csv':
        reportStage('Extracting CSV dataâ€¦');
        const csvResult = await extractCSV(
          file.buffer,
          (text) => embeddingService.estimateTokenCount(text),
          embeddingService.getModelMaxContextTokens()
        );
        content = csvResult.content;
        metadata = { ...metadata, ...csvResult.metadata };
        console.log('âœ“ CSV extracted successfully');
        break;

      case 'xlsx':
        reportStage('Extracting XLSX dataâ€¦');
        const xlsxResult = await extractXLSX(
          file.buffer,
          (text) => embeddingService.estimateTokenCount(text),
          embeddingService.getModelMaxContextTokens()
        );
        content = xlsxResult.content;
        metadata = { ...metadata, ...xlsxResult.metadata };
        console.log('âœ“ XLSX extracted successfully');
        break;

      case 'pptx':
        reportStage('Extracting PPTX slidesâ€¦');
        const pptxResult = await extractPPTX(
          file.buffer,
          (text) => embeddingService.estimateTokenCount(text),
          embeddingService.getModelMaxContextTokens()
        );
        content = pptxResult.content;
        metadata = { ...metadata, ...pptxResult.metadata };
        console.log('âœ“ PPTX extracted successfully');
        break;

      case 'rtf':
        reportStage('Extracting RTF textâ€¦');
        const rtfResult = await extractRTF(file.buffer);
        content = rtfResult.content;
        metadata = { ...metadata, ...rtfResult.metadata };
        console.log('âœ“ RTF extracted successfully');
        break;

      // Legacy Office formats - require LibreOffice conversion
      case 'doc':
      case 'ppt':
      case 'xls':
        reportStage(`Converting legacy ${fileExt.toUpperCase()}â€¦`);
        if (!libreOfficeConverter || !libreOfficeConverter.isEnabled()) {
          throw new Error(`Legacy .${fileExt} files require LibreOffice conversion. Set LIBREOFFICE_ENABLED=true in .env to enable support.`);
        }
        
        const legacyConverted = await libreOfficeConverter.convert(file.buffer, fileExt);
        
        // Recursively process the converted file
        const legacyFile = {
          originalname: filename.replace(`.${fileExt}`, `.${legacyConverted.ext}`),
          buffer: legacyConverted.buffer
        };
        
        const legacyResult = await processSingleFile(legacyFile, collectionName, autoCategorize, options);
        return legacyResult;

      // OpenDocument formats - require LibreOffice conversion
      case 'odt':
      case 'odp':
      case 'ods':
        reportStage(`Converting ${fileExt.toUpperCase()}â€¦`);
        if (!libreOfficeConverter || !libreOfficeConverter.isEnabled()) {
          throw new Error(`OpenDocument .${fileExt} files require LibreOffice conversion. Set LIBREOFFICE_ENABLED=true in .env to enable support.`);
        }
        
        const odfConverted = await libreOfficeConverter.convert(file.buffer, fileExt);
        
        // Recursively process the converted file
        const odfFile = {
          originalname: filename.replace(`.${fileExt}`, `.${odfConverted.ext}`),
          buffer: odfConverted.buffer
        };
        
        const odfResult = await processSingleFile(odfFile, collectionName, autoCategorize, options);
        return odfResult;

      case 'jpg':
      case 'jpeg':
      case 'png':
      case 'gif':
      case 'webp':
      case 'bmp':
        // Process image with vision model
        if (!visionEnabled || !visionService) {
          throw new Error('Vision processing is not enabled. Set VISION_MODEL_ENABLED=true in .env');
        }

        reportStage('Processing imageâ€¦');
        console.log('Processing image with vision model...');
        const mimeType = fileExt === 'jpg' ? 'image/jpeg' : `image/${fileExt}`;
        const visionResult = await visionService.processImage(file.buffer, mimeType, options);

        // Use extracted markdown content as document content
        content = visionResult.markdownContent;

        // Store vision-specific metadata
        metadata.detected_language = visionResult.language;
        metadata.description = visionResult.description;
        metadata.document_type = 'image';
        metadata.vision_processed = true;

        // Optionally store base64 image for future regeneration
        // metadata.image_data = file.buffer.toString('base64');

        console.log(`âœ“ Image processed: ${visionResult.language} detected`);
        break;

      default:
        const supportedTypes = ['txt', 'json', 'pdf', 'docx', 'csv', 'xlsx', 'pptx', 'rtf'];
        if (visionEnabled) {
          supportedTypes.push('jpg', 'jpeg', 'png', 'gif', 'webp', 'bmp');
        }
        if (libreOfficeConverter && libreOfficeConverter.isEnabled()) {
          supportedTypes.push('doc', 'ppt', 'xls', 'odt', 'odp', 'ods');
        }
        throw new Error(`Unsupported file type: ${fileExt}. Supported: ${supportedTypes.join(', ')}`);
    }

    if (!content || content.trim().length === 0) {
      throw new Error('File is empty or could not extract content');
    }

    throwIfAborted(options.signal);

    reportStage('Validating sizeâ€¦');

    // Embedding-size guard: fail fast before running additional processing
    // (PII, categorization, description generation, etc.) that may call Ollama.
    const estimatedTokens = embeddingService.estimateTokenCount(content);
    const modelMaxTokens = embeddingService.getModelMaxContextTokens();
    if (estimatedTokens > modelMaxTokens) {
      const errorMsg = `Document too large: ${estimatedTokens} tokens exceeds model limit of ${modelMaxTokens} tokens`;
      console.error(`âŒ ${errorMsg}`);
      throw new Error(errorMsg);
    }

    throwIfAborted(options.signal);

    // PII Detection - scan for sensitive information
    if (piiDetectionEnabled) {
      reportStage('Checking for PIIâ€¦');
      console.log('Scanning for PII...');
      const piiStartTime = Date.now();
      const piiResult = await piiService.detectPII(content, options);
      const piiDuration = ((Date.now() - piiStartTime) / 1000).toFixed(2);

      if (piiResult.hasPII) {
        metadata.pii_detected = true;
        metadata.pii_types = piiResult.piiTypes;
        metadata.pii_details = piiResult.piiDetails;
        metadata.pii_risk_level = piiResult.riskLevel;
        metadata.pii_scan_date = piiResult.scanTimestamp;
        metadata.pii_detection_method = piiResult.detectionMethod;

        console.log(`âš ï¸  PII detected: ${piiResult.piiTypes.join(', ')} (${piiResult.piiDetails.length} items) [${piiDuration}s]`);
      } else {
        metadata.pii_detected = false;
        metadata.pii_scan_date = piiResult.scanTimestamp;
        console.log(`âœ“ No PII detected [${piiDuration}s]`);
      }
    }

    // Automatic categorization if enabled and requested
    if (categorizationModel && autoCategorize) {
      reportStage('Categorizingâ€¦');
      console.log('Automatic categorization requested...');
      const catStartTime = Date.now();
      const extracted = await categorizationService.categorizeDocument(content, options);
      const catDuration = ((Date.now() - catStartTime) / 1000).toFixed(2);
      metadata = { ...metadata, ...extracted };
      console.log(`Categorization complete [${catDuration}s]:`, extracted);
    }

    throwIfAborted(options.signal);

    // Auto-generate description for non-image documents (images already have description from vision)
    if (autoGenerateDescription && descriptionService && !metadata.description) {
      reportStage('Generating descriptionâ€¦');
      console.log('Auto-generating description...');
      const descStartTime = Date.now();
      const descResult = await descriptionService.generateDescription(content, fileExt, options);
      const descDuration = ((Date.now() - descStartTime) / 1000).toFixed(2);
      metadata.detected_language = descResult.language;
      metadata.description = descResult.description;
      console.log(`âœ“ Description generated [${descDuration}s]: ${descResult.language}`);
    }

    throwIfAborted(options.signal);

    // Parse metadata from content if it's structured
    const parsedMetadata = parseMetadataFromContent(filename, content, metadata);
    parsedMetadata.file_type = fileExt;

    // Get dense embedding
    reportStage('Embeddingâ€¦');
    console.log(`Generating embedding for ${filename} (${content.length} chars, ~${estimatedTokens} tokens)...`);

    const embeddingStartTime = Date.now();
    const denseEmbedding = await embeddingService.getDenseEmbedding(content, options);
    const embeddingDuration = ((Date.now() - embeddingStartTime) / 1000).toFixed(2);
    if (!denseEmbedding) {
      throw new Error('Failed to generate embedding');
    }
    console.log(`âœ“ Embedding generated successfully (${embeddingDuration}s)`);

    // Generate sparse vector
    const sparseVector = getSparseVector(content);

    // Generate stable document hash for deduplication
    const documentHash = generateDocumentHash(filename, s3Key, driveId);

    // Check if document already exists (via existingDocuments map passed in options)
    const existingDocuments = options.existingDocuments || new Map();
    const existingDoc = existingDocuments.get(documentHash);

    let pointId = documentHash;
    const nowIso = new Date().toISOString();
    let addedAt = nowIso;
    let isUpdate = false;

    if (existingDoc) {
      // Document exists - preserve original added_at and use existing ID
      pointId = existingDoc.id;
      addedAt = existingDoc.added_at || addedAt;
      isUpdate = true;
      console.log(`ðŸ“ Updating existing document: ${filename} (ID: ${pointId})`);
    } else {
      console.log(`âž• Adding new document: ${filename} (ID: ${pointId})`);
    }

    const lastUpdated = isUpdate ? nowIso : addedAt;

    // Store in Qdrant (upsert handles both insert and update)
    reportStage('Saving to vector databaseâ€¦');
    await qdrantClient.upsert(collectionName, {
      points: [
        {
          id: pointId,
          vector: {
            dense: denseEmbedding,
            sparse: sparseVector
          },
          payload: {
            ...parsedMetadata,
            content: content,
            added_at: addedAt,
            last_updated: lastUpdated
          }
        }
      ]
    });

    console.log(`âœ… Successfully ${isUpdate ? 'updated' : 'uploaded'}: ${filename}`);

    reportStage('Done');

    return {
      filename,
      id: pointId,
      fileType: fileExt,
      contentLength: content.length,
      metadata: parsedMetadata,
      isUpdate: isUpdate
    };
  }

  async function addDocument({ collectionName, filename, content, metadata = {} }) {
    console.log(`Adding document: ${filename}`);

    if (!content || content.trim().length === 0) {
      throw new Error('Document content is empty');
    }

    // Embedding-size guard: fail fast before running additional processing
    // (PII, categorization, description generation, etc.) that may call Ollama.
    const estimatedTokens = embeddingService.estimateTokenCount(content);
    const modelMaxTokens = embeddingService.getModelMaxContextTokens();
    if (estimatedTokens > modelMaxTokens) {
      const errorMsg = `Document too large: ${estimatedTokens} tokens exceeds model limit of ${modelMaxTokens} tokens`;
      console.error(`âŒ ${errorMsg}`);
      throw new Error(errorMsg);
    }

    // PII Detection - scan for sensitive information
    if (piiDetectionEnabled) {
      console.log('Scanning for PII...');
      const piiResult = await piiService.detectPII(content);

      if (piiResult.hasPII) {
        metadata.pii_detected = true;
        metadata.pii_types = piiResult.piiTypes;
        metadata.pii_details = piiResult.piiDetails;
        metadata.pii_risk_level = piiResult.riskLevel;
        metadata.pii_scan_date = piiResult.scanTimestamp;
        metadata.pii_detection_method = piiResult.detectionMethod;

        console.log(`âš ï¸  PII detected: ${piiResult.piiTypes.join(', ')} (${piiResult.piiDetails.length} items)`);
      } else {
        metadata.pii_detected = false;
        metadata.pii_scan_date = piiResult.scanTimestamp;
        console.log('âœ“ No PII detected');
      }
    }

    // Parse metadata from content
    const parsedMetadata = parseMetadataFromContent(filename, content, metadata);

    // Get dense embedding
    const denseEmbedding = await embeddingService.getDenseEmbedding(content);
    if (!denseEmbedding) {
      throw new Error('Failed to generate embedding');
    }

    // Generate sparse vector
    const sparseVector = getSparseVector(content);

    const pointId = generateDocumentHash(filename);
    const now = new Date().toISOString();

    const existingDocuments = await checkForDuplicates(collectionName, [pointId]);
    const existingDoc = existingDocuments.get(pointId);

    const addedAt = existingDoc?.payload?.added_at || now;
    const lastUpdated = existingDoc ? now : addedAt;
    const isUpdate = !!existingDoc;

    // Store in Qdrant
    await qdrantClient.upsert(collectionName, {
      points: [
        {
          id: pointId,
          vector: {
            dense: denseEmbedding,
            sparse: sparseVector
          },
          payload: {
            ...parsedMetadata,
            content: content,
            added_at: addedAt,
            last_updated: lastUpdated
          }
        }
      ]
    });

    console.log(`âœ… Successfully ${isUpdate ? 'updated' : 'added'}: ${filename}`);

    return { pointId, parsedMetadata, isUpdate };
  }

  async function extractContentForSearchByDocument({ fileBuffer, filename }) {
    let content = '';
    const fileExt = filename.split('.').pop().toLowerCase();

    if (fileExt === 'txt' || fileExt === 'md') {
      content = fileBuffer.toString('utf-8');
    } else if (fileExt === 'pdf') {
      // Use the same PDF extraction logic as the main upload
      try {
        content = await pdfToMarkdownViaHtml(fileBuffer);
      } catch (htmlError) {
        console.warn('PDF via HTML conversion failed, trying @opendocsg/pdf2md:', htmlError.message);
        try {
          content = await pdf2md(fileBuffer);
        } catch (pdf2mdError) {
          console.warn('pdf2md failed, using basic text extraction:', pdf2mdError.message);
          const pdfData = await pdfParse(fileBuffer);
          content = processPdfText(pdfData.text);
        }
      }
    } else if (fileExt === 'docx') {
      // Use markdown conversion for better structure preservation
      const result = await mammoth.convertToMarkdown({ buffer: fileBuffer });
      content = result.value;
    } else if (fileExt === 'csv') {
      // CSV extraction
      const csvResult = await extractCSV(
        fileBuffer,
        (text) => embeddingService.estimateTokenCount(text),
        embeddingService.getModelMaxContextTokens()
      );
      content = csvResult.content;
    } else if (fileExt === 'xlsx') {
      // XLSX extraction
      const xlsxResult = await extractXLSX(
        fileBuffer,
        (text) => embeddingService.estimateTokenCount(text),
        embeddingService.getModelMaxContextTokens()
      );
      content = xlsxResult.content;
    } else if (fileExt === 'pptx') {
      // PPTX extraction
      const pptxResult = await extractPPTX(
        fileBuffer,
        (text) => embeddingService.estimateTokenCount(text),
        embeddingService.getModelMaxContextTokens()
      );
      content = pptxResult.content;
    } else if (fileExt === 'rtf') {
      // RTF extraction
      const rtfResult = await extractRTF(fileBuffer);
      content = rtfResult.content;
    } else if (['doc', 'ppt', 'xls', 'odt', 'odp', 'ods'].includes(fileExt)) {
      // Legacy/ODF formats require LibreOffice
      if (!libreOfficeConverter || !libreOfficeConverter.isEnabled()) {
        const err = new Error(`Legacy/OpenDocument .${fileExt} files require LibreOffice conversion. Set LIBREOFFICE_ENABLED=true in .env to enable support.`);
        err.code = 'UNSUPPORTED_FILE_TYPE';
        throw err;
      }
      
      // Convert and recursively extract
      const converted = await libreOfficeConverter.convert(fileBuffer, fileExt);
      return extractContentForSearchByDocument({
        fileBuffer: converted.buffer,
        filename: filename.replace(`.${fileExt}`, `.${converted.ext}`)
      });
    } else {
      const supportedTypes = ['txt', 'md', 'pdf', 'docx', 'csv', 'xlsx', 'pptx', 'rtf'];
      if (libreOfficeConverter && libreOfficeConverter.isEnabled()) {
        supportedTypes.push('doc', 'ppt', 'xls', 'odt', 'odp', 'ods');
      }
      const err = new Error(`Unsupported file type: ${fileExt}. Supported: ${supportedTypes.join(', ')}`);
      err.code = 'UNSUPPORTED_FILE_TYPE';
      throw err;
    }

    return { content, fileExt };
  }

  /**
   * Batch check for existing documents by their hashes.
   * Returns a Map of hash -> {id, added_at} for efficient lookup during upload.
   * 
   * @param {string} collectionName - Qdrant collection name
   * @param {number[]} documentHashes - Array of document hashes to check
   * @returns {Promise<Map<number, {id: number, added_at: string}>>}
   */
  async function checkForDuplicates(collectionName, documentHashes) {
    if (!documentHashes || documentHashes.length === 0) {
      return new Map();
    }

    console.log(`ðŸ” Checking for ${documentHashes.length} potential duplicates...`);

    try {
      // Query Qdrant by point IDs (fast, single call). Using IDs avoids payload filtering/index needs.
      const points = await qdrantClient.retrieve(collectionName, {
        ids: documentHashes,
        with_payload: true,
        with_vector: false
      });

      const existingMap = new Map();
      if (Array.isArray(points) && points.length > 0) {
        points.forEach(point => {
          existingMap.set(point.id, {
            id: point.id,
            added_at: point.payload?.added_at
          });
        });
        console.log(`ðŸ“‹ Found ${existingMap.size} existing documents that will be updated`);
      } else {
        console.log('âœ¨ No duplicates found - all files are new');
      }

      return existingMap;
    } catch (error) {
      console.error('Error checking for duplicates:', error);
      return new Map();
    }
  }

  return {
    processSingleFile,
    addDocument,
    extractContentForSearchByDocument,
    checkForDuplicates,
    visionService,
    descriptionService
  };
}

module.exports = {
  createDocumentService
};
